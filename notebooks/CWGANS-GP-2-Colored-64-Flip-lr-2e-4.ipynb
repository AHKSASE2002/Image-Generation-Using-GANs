{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "from torch.utils.data import DataLoader, Dataset, Subset, random_split\n",
    "from torchvision import transforms, datasets\n",
    "from torchvision.utils import make_grid\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import random\n",
    "from torchvision.utils import make_grid\n",
    "from tqdm import tqdm\n",
    "from scipy import linalg\n",
    "from torch.nn.functional import adaptive_avg_pool2d\n",
    "import torchvision.models as models\n",
    "from IPython.display import clear_output\n",
    "# from utils import gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = 2e-4\n",
    "batch_size = 64\n",
    "# image_size = 64\n",
    "image_size = 128\n",
    "# image_size = 130\n",
    "in_channels = 3\n",
    "z_dim = 100\n",
    "epochs = 3000\n",
    "features_disc = 64\n",
    "features_gen = 64\n",
    "dropout = 0\n",
    "critic_iter = 5\n",
    "lambda_gp = 10\n",
    "# weight_clip = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir = \"../../Shoes-Dataset-Colored/\"\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class CustomRotate:\n",
    "#     def __init__(self, degrees, fill=(255, 255, 255)):\n",
    "#         self.degrees = degrees\n",
    "#         self.fill = fill\n",
    "\n",
    "#     def __call__(self, img):\n",
    "#         # Convert PIL image to numpy array\n",
    "#         img_np = np.array(img)\n",
    "#         # Rotate the image\n",
    "#         rotated_img = Image.fromarray(np.uint8(img_np)).rotate(self.degrees, fillcolor=self.fill)\n",
    "#         return rotated_img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rotate_probability = 0.4 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conditional_rotation(image):\n",
    "#     if random.random() < rotate_probability:\n",
    "#         return CustomRotate(degrees=random.randint(0,225))(image)\n",
    "#     else:\n",
    "#         return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((image_size, image_size)),\n",
    "    transforms.RandomHorizontalFlip(),             \n",
    "    # transforms.Lambda(conditional_rotation),  # Conditional rotation\n",
    "    # transforms.ColorJitter(brightness=0.2,         \n",
    "    #                        saturation=0,\n",
    "    #                        contrast=0,\n",
    "    #                        hue=0.1),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5 for _ in range(in_channels)], [0.5 for _ in range(in_channels)])\n",
    "])\n",
    "footwear = datasets.ImageFolder(root=dir, transform=transform)\n",
    "mapping = footwear.class_to_idx\n",
    "mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(footwear, batch_size=batch_size, shuffle=True, drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_tensor_images(image_tensor, num_images=1, size=(1, 28, 28)):\n",
    "    image_tensor = (image_tensor + 1) / 2\n",
    "    image_unflat = image_tensor.detach().cpu()\n",
    "    image_grid = make_grid(image_unflat[:num_images], nrow=5)\n",
    "    plt.imshow(image_grid.permute(1, 2, 0).squeeze())\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "show_tensor_images(next(iter(train_loader))[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Critic(nn.Module):\n",
    "    \n",
    "    def __init__(self, channel_imgs, features_d, num_of_classes, img_size, dropout_prob) -> None:\n",
    "        super(Critic, self).__init__()\n",
    "        # Input shape will be N x channel_imgs x 64x64\n",
    "        self.img_size = img_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.critic = nn.Sequential(\n",
    "            \n",
    "            nn.Conv2d(channel_imgs+1, features_d, kernel_size=4, stride=2, padding=1), # 32x32\n",
    "            nn.LeakyReLU(0.2),\n",
    "            self._block(features_d, features_d*2, 4, 2, 1), # 16x16\n",
    "            self._block(features_d*2, features_d*4, 4, 2, 1), # 8x8\n",
    "            self._block(features_d*4, features_d*8, 4, 2, 1), # 4x4\n",
    "            nn.Conv2d(features_d*8, out_channels= 1, kernel_size=4, stride=2, padding=1), # 1 x 1\n",
    "            \n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_of_classes, img_size * img_size)\n",
    "    \n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size, stride, padding),\n",
    "            nn.InstanceNorm2d(out_channels, affine=True),\n",
    "            nn.LeakyReLU(0.2),\n",
    "        )\n",
    "        \n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embedding(labels).view(labels.shape[0], 1, self.img_size, self.img_size)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        return self.critic(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    def __init__(self, z_dim, channels_img, features_g, num_classes, img_size, embed_size, dropout_prob = 0.5) -> None:\n",
    "        super(Generator, self).__init__()\n",
    "        self.img_size = img_size\n",
    "        self.dropout_prob = dropout_prob\n",
    "        self.generator = nn.Sequential(\n",
    "            # Input: N x z_dim x 1x1\n",
    "            self._block(z_dim + embed_size, features_g*16, 4, 1, 0), # N x features_g*16 x 16 \n",
    "            self._block(features_g*16, features_g*8, 4, 2, 1),  # N x features_g*8 x 8x8\n",
    "            self._block(features_g*8, features_g*4, 4, 2, 1),  # N x features_g*4 x 16x16\n",
    "            self._block(features_g*4, features_g*2, 4, 2, 1),  # N x features_g*2 x 32x32\n",
    "            nn.ConvTranspose2d(features_g*2, channels_img, kernel_size=4, stride=2, padding=1), # N x channels_img x 64x64\n",
    "            nn.Tanh(), # [-1, 1]       \n",
    "        )\n",
    "        self.embedding = nn.Embedding(num_classes, embed_size)\n",
    "    def _block(self, in_channels, out_channels, kernel_size, stride, padding):\n",
    "        return nn.Sequential(\n",
    "            nn.ConvTranspose2d(in_channels, out_channels, kernel_size, stride, padding), # deconvolution\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "    def forward(self, x, labels):\n",
    "        embedding = self.embedding(labels).unsqueeze(2).unsqueeze(3)\n",
    "        x = torch.cat([x, embedding], dim=1)\n",
    "        \n",
    "        return self.generator(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def intialize_weights(model):\n",
    "    for m in model.modules():\n",
    "        if isinstance(m, (nn.Conv2d, nn.ConvTranspose2d, nn.BatchNorm2d)):\n",
    "            nn.init.normal_(m.weight.data, 0.0, 0.02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_penalty(critic, labels,real, fake, device=\"cuda\"):\n",
    "    batch_size, C, H, W = real.shape\n",
    "    epsilon = torch.rand((batch_size, 1, 1, 1)).repeat(1, C, H, W).to(device)\n",
    "    interpolated_images = real * epsilon + fake * (1 - epsilon)\n",
    "    \n",
    "    mixed_scores = critic(interpolated_images, labels)\n",
    "    \n",
    "    gradient = torch.autograd.grad(\n",
    "        inputs=interpolated_images,\n",
    "        outputs=mixed_scores,\n",
    "        grad_outputs=torch.ones_like(mixed_scores),\n",
    "        create_graph=True,\n",
    "        retain_graph=True,\n",
    "    )[0]\n",
    "    gradient = gradient.view(gradient.shape[0], -1)\n",
    "    gradient_norm = gradient.norm(2, dim=1)\n",
    "    gradient_penalty = torch.mean((gradient_norm - 1) ** 2)\n",
    "    \n",
    "    return gradient_penalty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 3\n",
    "gen_embed = 100\n",
    "# gen_embed = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen = Generator(z_dim, in_channels, features_gen, num_classes, image_size, gen_embed, dropout_prob= dropout).to(device)\n",
    "critic = Critic(in_channels, features_disc, num_classes, image_size, dropout_prob= dropout).to(device)\n",
    "intialize_weights(gen)\n",
    "intialize_weights(critic)\n",
    "# opt_gen = optim.RMSprop(gen.parameters(), lr=lr)\n",
    "# opt_critic = optim.RMSprop(critic.parameters(), lr=lr)\n",
    "\n",
    "opt_gen = optim.Adam(gen.parameters(), lr=lr, betas=(0.0, 0.9))\n",
    "opt_critic = optim.Adam(critic.parameters(), lr=lr, betas=(0.0, 0.9))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fixed_noise = torch.randn(32, z_dim, 1, 1)\n",
    "writer_real = SummaryWriter(f\"logs/real\")\n",
    "write_fake = SummaryWriter(f\"logs/fake\")\n",
    "step = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen.load_state_dict(torch.load(\"./model_500/gen_500.pth\"))\n",
    "# disc.load_state_dict(torch.load(\"./model_500/disc_500.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "critic.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InceptionV3(nn.Module):\n",
    "    \"\"\"Pretrained InceptionV3 network returning feature maps\"\"\"\n",
    "\n",
    "    # Index of default block of inception to return,\n",
    "    # corresponds to output of final average pooling\n",
    "    DEFAULT_BLOCK_INDEX = 3\n",
    "\n",
    "    # Maps feature dimensionality to their output blocks indices\n",
    "    BLOCK_INDEX_BY_DIM = {\n",
    "        64: 0,   # First max pooling features\n",
    "        192: 1,  # Second max pooling featurs\n",
    "        768: 2,  # Pre-aux classifier features\n",
    "        2048: 3  # Final average pooling features\n",
    "    }\n",
    "\n",
    "    def __init__(self,\n",
    "                 output_blocks=[DEFAULT_BLOCK_INDEX],\n",
    "                 resize_input=True,\n",
    "                 normalize_input=True,\n",
    "                 requires_grad=False):\n",
    "        \n",
    "        super(InceptionV3, self).__init__()\n",
    "\n",
    "        self.resize_input = resize_input\n",
    "        self.normalize_input = normalize_input\n",
    "        self.output_blocks = sorted(output_blocks)\n",
    "        self.last_needed_block = max(output_blocks)\n",
    "\n",
    "        assert self.last_needed_block <= 3, \\\n",
    "            'Last possible output block index is 3'\n",
    "\n",
    "        self.blocks = nn.ModuleList()\n",
    "\n",
    "        \n",
    "        inception = models.inception_v3(pretrained=True)\n",
    "\n",
    "        # Block 0: input to maxpool1\n",
    "        block0 = [\n",
    "            inception.Conv2d_1a_3x3,\n",
    "            inception.Conv2d_2a_3x3,\n",
    "            inception.Conv2d_2b_3x3,\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "        ]\n",
    "        self.blocks.append(nn.Sequential(*block0))\n",
    "\n",
    "        # Block 1: maxpool1 to maxpool2\n",
    "        if self.last_needed_block >= 1:\n",
    "            block1 = [\n",
    "                inception.Conv2d_3b_1x1,\n",
    "                inception.Conv2d_4a_3x3,\n",
    "                nn.MaxPool2d(kernel_size=3, stride=2)\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block1))\n",
    "\n",
    "        # Block 2: maxpool2 to aux classifier\n",
    "        if self.last_needed_block >= 2:\n",
    "            block2 = [\n",
    "                inception.Mixed_5b,\n",
    "                inception.Mixed_5c,\n",
    "                inception.Mixed_5d,\n",
    "                inception.Mixed_6a,\n",
    "                inception.Mixed_6b,\n",
    "                inception.Mixed_6c,\n",
    "                inception.Mixed_6d,\n",
    "                inception.Mixed_6e,\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block2))\n",
    "\n",
    "        # Block 3: aux classifier to final avgpool\n",
    "        if self.last_needed_block >= 3:\n",
    "            block3 = [\n",
    "                inception.Mixed_7a,\n",
    "                inception.Mixed_7b,\n",
    "                inception.Mixed_7c,\n",
    "                nn.AdaptiveAvgPool2d(output_size=(1, 1))\n",
    "            ]\n",
    "            self.blocks.append(nn.Sequential(*block3))\n",
    "\n",
    "        for param in self.parameters():\n",
    "            param.requires_grad = requires_grad\n",
    "\n",
    "    def forward(self, inp):\n",
    "        \"\"\"Get Inception feature maps\n",
    "        Parameters\n",
    "        ----------\n",
    "        inp : torch.autograd.Variable\n",
    "            Input tensor of shape Bx3xHxW. Values are expected to be in\n",
    "            range (0, 1)\n",
    "        Returns\n",
    "        -------\n",
    "        List of torch.autograd.Variable, corresponding to the selected output\n",
    "        block, sorted ascending by index\n",
    "        \"\"\"\n",
    "        outp = []\n",
    "        x = inp\n",
    "\n",
    "        if self.resize_input:\n",
    "            x = F.interpolate(x,\n",
    "                              size=(299, 299),\n",
    "                              mode='bilinear',\n",
    "                              align_corners=False)\n",
    "\n",
    "        if self.normalize_input:\n",
    "            x = 2 * x - 1  # Scale from range (0, 1) to range (-1, 1)\n",
    "\n",
    "        for idx, block in enumerate(self.blocks):\n",
    "            x = block(x)\n",
    "            if idx in self.output_blocks:\n",
    "                outp.append(x)\n",
    "\n",
    "            if idx == self.last_needed_block:\n",
    "                break\n",
    "\n",
    "        return outp\n",
    "    \n",
    "block_idx = InceptionV3.BLOCK_INDEX_BY_DIM[2048]\n",
    "model = InceptionV3([block_idx])\n",
    "model=model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_activation_statistics(images,model,batch_size=128, dims=2048,\n",
    "                    cuda=False):\n",
    "    model.eval()\n",
    "    act=np.empty((len(images), dims))\n",
    "    \n",
    "    if cuda:\n",
    "        batch=images.cuda()\n",
    "    else:\n",
    "        batch=images\n",
    "    pred = model(batch)[0]\n",
    "\n",
    "        # If model output is not scalar, apply global spatial average pooling.\n",
    "        # This happens if you choose a dimensionality not equal 2048.\n",
    "    if pred.size(2) != 1 or pred.size(3) != 1:\n",
    "        pred = adaptive_avg_pool2d(pred, output_size=(1, 1))\n",
    "\n",
    "    act= pred.cpu().data.numpy().reshape(pred.size(0), -1)\n",
    "    \n",
    "    mu = np.mean(act, axis=0)\n",
    "    sigma = np.cov(act, rowvar=False)\n",
    "    return mu, sigma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_frechet_distance(mu1, sigma1, mu2, sigma2, eps=1e-6):\n",
    "    \"\"\"Numpy implementation of the Frechet Distance.\n",
    "    The Frechet distance between two multivariate Gaussians X_1 ~ N(mu_1, C_1)\n",
    "    and X_2 ~ N(mu_2, C_2) is\n",
    "            d^2 = ||mu_1 - mu_2||^2 + Tr(C_1 + C_2 - 2*sqrt(C_1*C_2)).\n",
    "    \"\"\"\n",
    "\n",
    "    mu1 = np.atleast_1d(mu1)\n",
    "    mu2 = np.atleast_1d(mu2)\n",
    "\n",
    "    sigma1 = np.atleast_2d(sigma1)\n",
    "    sigma2 = np.atleast_2d(sigma2)\n",
    "\n",
    "    assert mu1.shape == mu2.shape, \\\n",
    "        'Training and test mean vectors have different lengths'\n",
    "    assert sigma1.shape == sigma2.shape, \\\n",
    "        'Training and test covariances have different dimensions'\n",
    "\n",
    "    diff = mu1 - mu2\n",
    "\n",
    "    \n",
    "    covmean, _ = linalg.sqrtm(sigma1.dot(sigma2), disp=False)\n",
    "    if not np.isfinite(covmean).all():\n",
    "        msg = ('fid calculation produces singular product; '\n",
    "               'adding %s to diagonal of cov estimates') % eps\n",
    "        print(msg)\n",
    "        offset = np.eye(sigma1.shape[0]) * eps\n",
    "        covmean = linalg.sqrtm((sigma1 + offset).dot(sigma2 + offset))\n",
    "\n",
    "    \n",
    "    if np.iscomplexobj(covmean):\n",
    "        if not np.allclose(np.diagonal(covmean).imag, 0, atol=1e-3):\n",
    "            m = np.max(np.abs(covmean.imag))\n",
    "            raise ValueError('Imaginary component {}'.format(m))\n",
    "        covmean = covmean.real\n",
    "\n",
    "    tr_covmean = np.trace(covmean)\n",
    "\n",
    "    return (diff.dot(diff) + np.trace(sigma1) +\n",
    "            np.trace(sigma2) - 2 * tr_covmean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fretchet(images_real,images_fake,model):\n",
    "     mu_1,std_1=calculate_activation_statistics(images_real,model,cuda=True)\n",
    "     mu_2,std_2=calculate_activation_statistics(images_fake,model,cuda=True)\n",
    "    \n",
    "     \"\"\"get fretched distance\"\"\"\n",
    "     fid_value = calculate_frechet_distance(mu_1, std_1, mu_2, std_2)\n",
    "     return fid_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gen.load_state_dict(torch.load(\"./WGANS-GP-2_lr_2e-4/WGANS-GP-2_400/gen_400.pth\"))\n",
    "\n",
    "# gen.eval()\n",
    "\n",
    "# # infere some images\n",
    "# label = torch.tensor([0], device=device).reshape()\n",
    "# noise = torch.randn(1, z_dim, 1, 1, device=device)\n",
    "# fake = gen(noise, label)\n",
    "# # enhance the image\n",
    "# fake = transforms.Resize((150, 150))(fake)\n",
    "# show_tensor_images(fake, size=(3, 64, 64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This Cell Can be modified many times according to where the model stopped training\n",
    "gen.load_state_dict(torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/gen_149.pth\"))\n",
    "critic.load_state_dict(torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/critic_149.pth\"))\n",
    "opt_critic.load_state_dict(torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/opt_critic_149.pth\"))\n",
    "opt_gen.load_state_dict(torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/opt_gen_149.pth\"))\n",
    "loss_critic = torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/losses_critic.pth\")\n",
    "loss_gen = torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/losses_gen.pth\")\n",
    "fid_scores_prev = torch.load(\"./WGANS-GP-2_/WGANS-GP-2_149/fid_scores.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_epoch = 0\n",
    "step = 0\n",
    "losses_critic = []\n",
    "losses_gen = []\n",
    "fid_scores = []\n",
    "\n",
    "for epoch in range(start_epoch, epochs):\n",
    "    tqdm_train_loader = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "    for batch_idx, (real, labels) in enumerate(tqdm_train_loader):\n",
    "        labels = labels.to(device)\n",
    "        real = real.to(device)\n",
    "        cur_batch_size = real.shape[0]\n",
    "        for _ in range(critic_iter):\n",
    "            noise = torch.randn(cur_batch_size, z_dim, 1, 1).to(device)\n",
    "            fake = gen(noise, labels).to(device)\n",
    "            critic_real = critic(real, labels).reshape(-1)\n",
    "            critic_fake = critic(fake, labels).reshape(-1)\n",
    "            gp = gradient_penalty(critic, labels, real, fake, device=device)\n",
    "            loss_critic = (\n",
    "                -(torch.mean(critic_real) - torch.mean(critic_fake)) + lambda_gp * gp # <- remove lambda_gp if not using WGAN-GP\n",
    "                )\n",
    "            critic.zero_grad()\n",
    "            loss_critic.backward(retain_graph=True)\n",
    "            opt_critic.step()\n",
    "        \n",
    "        losses_critic.append(loss_critic.item())\n",
    "\n",
    "        # for p in critic.parameters():\n",
    "        #     p.data.clamp_(-weight_clip, weight_clip)\n",
    "            \n",
    "        \n",
    "        ## Train Generator: min -E[critic(gen_fake)]\n",
    "        output = critic(fake, labels).reshape(-1)\n",
    "        loss_gen = -torch.mean(output)\n",
    "        gen.zero_grad()\n",
    "        loss_gen.backward()\n",
    "        opt_gen.step()\n",
    "        losses_gen.append(loss_gen.item())\n",
    "\n",
    "        # Update progress bar description\n",
    "        tqdm_train_loader.set_postfix({\"Loss D\": loss_critic.item(), \"Loss G\": loss_gen.item()})\n",
    "\n",
    "        if batch_idx % 100 == 0:\n",
    "            tqdm_train_loader.write(\n",
    "                f\"Epoch [{epoch+1}/{epochs}] Batch {batch_idx}/{len(train_loader)} Loss D: {loss_critic:.4f}, loss G: {loss_gen:.4f}\"\n",
    "            )\n",
    "\n",
    "            with torch.no_grad():\n",
    "                fake = gen(noise, labels)\n",
    "                img_grid_real = make_grid(real[:32], normalize=True)\n",
    "                img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "                writer_real.add_image(\"Real\", img_grid_real, global_step=step)\n",
    "                write_fake.add_image(\"Fake\", img_grid_fake, global_step=step)\n",
    "                step += 1\n",
    "\n",
    "    # Visualize and print losses after each epoch\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.plot(losses_critic, label='Critic Loss')\n",
    "    plt.plot(losses_gen, label='Generator Loss')\n",
    "    plt.xlabel('Iteration')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Epoch {epoch+1}/{epochs}: Critic and Generator Losses')\n",
    "    plt.legend()\n",
    "    os.makedirs(\"images/loss\", exist_ok=True)\n",
    "    plt.savefig(f\"images/loss/loss{epoch+1}.png\")\n",
    "    plt.close()\n",
    "    # Calculate and visualize FID every epoch\n",
    "    with torch.no_grad():\n",
    "        fake = gen(noise, labels)\n",
    "        fid = calculate_fretchet(real, fake, model)\n",
    "        fid_scores.append(fid)\n",
    "\n",
    "        # Visualize FID\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID')\n",
    "        plt.title('FID Score')\n",
    "        plt.legend()\n",
    "        os.makedirs(\"images/fid\", exist_ok=True)\n",
    "        plt.savefig(f\"images/fid/fid{epoch+1}.png\")\n",
    "        plt.close()\n",
    "\n",
    "        # Print FID score\n",
    "        print(f\"Epoch [{epoch+1}/{epochs}] FID: {fid:.4f}\")\n",
    "\n",
    "    if (epoch+1) % 1 == 0 or epoch == 0:\n",
    "            gen.eval()\n",
    "            noise = torch.randn(32, z_dim, 1, 1).to(device)\n",
    "            fake = gen(noise, labels[:32])\n",
    "            img_grid_fake = make_grid(fake[:32], normalize=True)\n",
    "            plt.imshow(img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "            os.makedirs(\"images/fake\", exist_ok=True)\n",
    "            plt.savefig(f\"images/fake/fake{epoch+1}.png\")\n",
    "            plt.close()\n",
    "\n",
    "    # Save evaluation metrics, generator, Critic, and optimizers every 50 epochs\n",
    "    if (epoch + 1) % 50 == 0:\n",
    "        os.makedirs(f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}\", exist_ok=True)\n",
    "        \n",
    "        # Visualize FID\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(range(epoch + 1), fid_scores, label='FID')\n",
    "        plt.xlabel('Epoch')\n",
    "        plt.ylabel('FID')\n",
    "        plt.title('FID Score')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/fid_plot.png\")\n",
    "        plt.close()\n",
    "        \n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(losses_critic, label='Critic Loss')\n",
    "        plt.plot(losses_gen, label='Generator Loss')\n",
    "        plt.xlabel('Iteration')\n",
    "        plt.ylabel('Loss')\n",
    "        plt.title(f'Epoch {epoch+1}/{epochs}: Critic and Generator Losses')\n",
    "        plt.legend()\n",
    "        plt.savefig(f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/losses_plot.png\")  \n",
    "        \n",
    "        plt.imsave(f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/fake_{epoch+1}.png\", img_grid_fake.permute(1, 2, 0).cpu().numpy())\n",
    "        plt.close()\n",
    "        # Save evaluation metrics\n",
    "        evaluation_metrics = {\"FID\": fid_scores[-1]}\n",
    "        torch.save(evaluation_metrics, f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/evaluation_metrics.pth\")\n",
    "\n",
    "        # Save generator, Critic, and optimizers\n",
    "        torch.save(gen.state_dict(), f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/gen_{epoch+1}.pth\")\n",
    "        torch.save(critic.state_dict(), f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/critic_{epoch+1}.pth\")\n",
    "        torch.save(opt_gen.state_dict(), f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/opt_gen_{epoch+1}.pth\")\n",
    "        torch.save(opt_critic.state_dict(), f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/opt_critic_{epoch+1}.pth\")\n",
    "\n",
    "        # Save losses and evaluation scores to files\n",
    "        torch.save(losses_critic, f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/losses_critic.pth\")\n",
    "        torch.save(losses_gen, f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/losses_gen.pth\")\n",
    "        torch.save(fid_scores, f\"WGANS-GP-2_/WGANS-GP-2_{epoch+1}/fid_scores.pth\")\n",
    "    if (epoch+1) %25 == 0:\n",
    "        clear_output()\n",
    "    gen.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
